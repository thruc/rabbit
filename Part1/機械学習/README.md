# 機械学習
コンピュータプログラムは、タスクT(アプリケーションにさせたいこと)を性能指標Pで測定し、その性能が経験E(データ)により改善される場合、タスクTおよび性能指標Pに関して経験 Eから学習すると言われている

例）  株価の場合
タスクT→入力された過去の株価のデータを入力すると次の株価を予測する
性能指標P→予測した株価と実際の株価の差分
経験E→過去の株価のデータ  


## 回帰問題  
ある離散あるいは連続値の入力から連続値の出力で予測する問題  

- 直線で予測する場合→線形回帰問題
- 曲線で予測する場合→非線形回帰問題 

## 回帰問題で扱うデータ  
- 入力（各要素の説明変数または特徴量と呼ぶ)
    - m次元のベクトル 
- 出力（目的変数） 
    - スカラー値

説明変数:$x$ = ($x_1$ ,$x_2$ ,$\dots$,$x_m$)$^T$ $\in$ $\mathbb{R}^m$ 
目的変数:$y$=$\in$ $\mathbb{R}^m$ 。

例）住宅価格予測 
説明変数：部屋数、敷地面積や築年数 
目的変数：価格 

## 線形回帰 
- 回帰問題を解くための機械学習モデルのひとつ
- 教師あり学習　
- 入力とm次元のパラメータの線形結合を出力するモデル

線形結合(入力とパラメータの内積)

パラメータ： $w$ = ($w_1$ ,$w_2$ ,・・・,$w_m$)$^T$ $\subset$ $\mathbb{R}^m$ 
説明変数：$x$ = ($x_1$ ,$x_2$ ,・・・,$x_m$)$^T$ $\subset$ $\mathbb{R}^m$ 
予測値: $\hat{y}$  　
線形結合：

```math
\hat{y} = w^Tx + w_0= \sum_{j=1}^{m} w_jx_j + w_0 
```


## データ分割
モデルの汎化性能を測定するため、データを学習用データと検証用データに分割する 
モデルの汎化性能(Generalization)を測定するためデータへの当てはまりの良さあまり意味がない、未知のデータに対してどれくらいうまく予測できるかがしたいこと 

## 学習(最小二乗法)
### 平均二乗誤差 
データとモデル出力の二乗誤差

```math
MSE_{train} = \frac{1}{n_{train}}\sum_{i=1}^{n_{train}}(\hat{y}_i^{(train)}-y_i^{(train)})^2
```
 
### 最小二乗法 
- 学習データの平均二乗誤差を最小とするパラメータを探索する
- 学習データの平均二乗誤差を最小は、その勾配が０になる点を求めれば良い 

MSEを微分して0となるように解いていく$\hat{W}$(回帰係数)が求められる

```math
\hat{W} = (X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}
```
よって予測値$\hat{y}$は 

```math
\hat{y}=X\hat{W} = X(X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}
```
となる
## 線形回帰演習   
https://github.com/Tomo-Horiuchi/rabbit/blob/feature/ml/Part1/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/01_%E7%B7%9A%E5%9E%8B%E5%9B%9E%E5%B8%B0%E3%83%A2%E3%83%87%E3%83%AB/skl_regression.ipynb 
モデル評価法の参考
https://funatsu-lab.github.io/open-course-ware/basic-theory/accuracy-index/#r2
### 演習実施結果 
データを7対3で分けてそれぞれ分析し決定係数と平均二乗誤差を出しました
  
- 線形単回帰分析 

説明変数：部屋数
目的変数：価格

MSE Train : 44.983, Test : 40.412 
$R^2$ Train : 0.500, Test : 0.434  

- 重回帰分析(2変数) 
  
説明変数：部屋数、犯罪率 
目的変数：価格  
MSE Train : 40.586, Test : 34.377
$R^2$ Train : 0.549, Test : 0.518

### 考察
MSEがより0に近く,$R^2$の値が1に近いので重回帰分析の方が精度が良いことがわかる



## 非線形回帰 
複数な非線形構造を内在する現象に対して、非線形回帰モデリングを実施する必要がある 

- 線形型のデータは限られている
- 非線形なデータに対して線形モデルだと説得力がない 
- 非線形な構造を捉える仕組みが必要

## 基底展開法 
非線形の場合、基底展開法という方法を使ってモデリングを行う 

- 回帰関数として、基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線形結合を利用
- 未知パラメータは、線形回帰モデルと同様に最小二乗法や最尤法による推定する。
- よく使われる基底関数 
    - 多項式関数 
    - ガウス型基底関数 
    - スプライン関数
 
```math
y_i=f(x_i)+\epsilon_i\qquad y_i=\omega_0+\sum_{i=1}^m\omega_j\phi_j(x_i)+\epsilon_i
```
$x$を線形の写像の$\phi$によって非線形化してから線形結合を見る 
ここで $\phi$が基底関数となる


### 多項式基底関数


```math
\phi_j=x^j
``` 
### ガウス型基底関数 

```math
\phi_j(x)=\exp\Biggl(\frac{(x-\mu_j)^2}{2h_j} \Biggr)
```


## モデル式
基底展開法も線形回帰と同じ枠組みで推定可能

説明変数:

```math
x_i=（x_{i1},x_{i2},\dots,x_{im})\in \mathbb{R}_m
```
非線形関数ベクトル:

```math
\phi(xi)=（\phi_1(x_i),\phi_2(x_i),\dots,\phi_k(x_i))^T∈\mathbb{R}^k
```
非線形関数の計画行列:

```math
\Phi_(train)=(\Phi(x_1),\Phi(x_2),\dots,\Phi(x_n))^T\in \mathbb{R}^{n×k}
```
最尤法による予測値:

```math
\hat{y}=\Phi(\Phi_(train)^T\Phi_(train))^{-1}\Phi_(train)^Ty_(train)
```
##未学習と過学習
### 未学習(underfitting)
学習データに対して、十分小さな誤差が得られないモデル 

対策 

- 表現力の高いモデルを利用する。


### 過学習（overfitting） 
小さな誤差は得られたが、テスト集合誤差との差が大きいモデル 
学習データに対して、小さな誤差が得られたが、検証データに対して、誤差が大きくなってしまう 

対策 

- 学習データの数を増やす 
- 不要な基底関数を削除して表現力を抑止 
- 正則化法を利用して表現力を抑止など

## 正則化法
モデルの複雑さに伴って、その値が大きくなる正則化項（罰則項）を課した関数を最小化

$S_γ=(y−\Phi w)^T(y−\Phi w)+γR(w)$  
正則化項 :$γR(w)$
形状によっていくつもの種類があり、それぞれ推定量の性質が異なる。

$γ$：正則化パラメータ 
モデルの曲線の滑らかさを調節 

## Ridge
正則化項にL2ノルムを使用したものをRidge推定 
パラメータを0に近づけるように推定
縮小推定と呼ばれる

## Lasso回帰
正則化項にL1ノルムを使用したものをLasso推定 
いくつかのパラメータを正確に0に推定
スパース推定と呼ばれる

## ホールドアウト法とクロスバリデーション法（交差検証）
### ホールドアウト法
データを学習用とテスト用の2つに分割し、予測精度や誤り率を推定するために使用 
手元に大量にデータがない場合、良い性能評価を与えないという欠点がある 
基底展開法に基づく非線形回帰モデルでは、基底関数の数、位置、チューニングを 
ホールドアウト値を小さくするモデルで決定する 

### クロスバリデーション法（交差検証）
データを学習用と検証用に分割し、検証用データを評価し精度の平均をCV値と呼ぶ
ホールドアウト法より、評価結果の信頼が高くなる。
## 非線形回帰演習   
正解の関数は $y=1-48x+218x^2-315x^3+145x^4$ とした

正解の関数にランダムにノイズを加えデータを生成
データから正解の関数をそれぞれ線形回帰と非線形回帰によって求める

## 演習結果
線形回帰の精度：0.3901811689763751
非線形回帰の精度：0.8824933990551088 
基底関数はR Eから学習すると言われている

例）  株価の場合
タスクT→入力された過去の株価のデータを入力すると次の株価を予測する
性能指標P→予測した株価と実際の株価の差分
経験E→過去の株価のデータ  


## 回帰問題  
ある離散あるいは連続値の入力から連続値の出力で予測する問題  

- 直線で予測する場合→線形回帰問題
- 曲線で予測する場合→非線形回帰問題 

## 回帰問題で扱うデータ  
- 入力（各要素の説明変数または特徴量と呼ぶ)
    - m次元のベクトル 
- 出力（目的変数） 
    - スカラー値

説明変数:$x$ = ($x_1$ ,$x_2$ ,$\dots$,$x_m$)$^T$ $\in$ $\mathbb{R}^m$ 
目的変数:$y$=$\in$ $\mathbb{R}^m$ 。

例）住宅価格予測 
説明変数：部屋数、敷地面積や築年数 
目的変数：価格 

## 線形回帰 
- 回帰問題を解くための機械学習モデルのひとつ
- 教師あり学習　
- 入力とm次元のパラメータの線形結合を出力するモデル

線形結合(入力とパラメータの内積)

パラメータ： $w$ = ($w_1$ ,$w_2$ ,・・・,$w_m$)$^T$ $\subset$ $\mathbb{R}^m$ 
説明変数：$x$ = ($x_1$ ,$x_2$ ,・・・,$x_m$)$^T$ $\subset$ $\mathbb{R}^m$ 
予測値: $\hat{y}$  　
線形結合：

```math
\hat{y} = w^Tx + w_0= \sum_{j=1}^{m} w_jx_j + w_0 
```


## データ分割
モデルの汎化性能を測定するため、データを学習用データと検証用データに分割する 
モデルの汎化性能(Generalization)を測定するためデータへの当てはまりの良さあまり意味がない、未知のデータに対してどれくらいうまく予測できるかがしたいこと 

## 学習(最小二乗法)
### 平均二乗誤差 
データとモデル出力の二乗誤差

```math
MSE_{train} = \frac{1}{n_{train}}\sum_{i=1}^{n_{train}}(\hat{y}_i^{(train)}-y_i^{(train)})^2
```
 
### 最小二乗法 
- 学習データの平均二乗誤差を最小とするパラメータを探索する
- 学習データの平均二乗誤差を最小は、その勾配が０になる点を求めれば良い 

MSEを微分して0となるように解いていく$\hat{W}$(回帰係数)が求められる

```math
\hat{W} = (X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}
```
よって予測値$\hat{y}$は 

```math
\hat{y}=X\hat{W} = X(X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}
```
となる
## 線形回帰演習   
https://github.com/Tomo-Horiuchi/rabbit/blob/feature/ml/Part1/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/01_%E7%B7%9A%E5%9E%8B%E5%9B%9E%E5%B8%B0%E3%83%A2%E3%83%87%E3%83%AB/skl_regression.ipynb 
モデル評価法の参考
https://funatsu-lab.github.io/open-course-ware/basic-theory/accuracy-index/#r2
### 演習実施結果 
データを7対3で分けてそれぞれ分析し決定係数と平均二乗誤差を出しました
  
- 線形単回帰分析 

説明変数：部屋数
目的変数：価格

MSE Train : 44.983, Test : 40.412 
$R^2$ Train : 0.500, Test : 0.434  

- 重回帰分析(2変数) 
  
説明変数：部屋数、犯罪率 
目的変数：価格  
MSE Train : 40.586, Test : 34.377
$R^2$ Train : 0.549, Test : 0.518

### 考察
MSEがより0に近く,$R^2$の値が1に近いので重回帰分析の方が精度が良いことがわかる



## 非線形回帰 
複数な非線形構造を内在する現象に対して、非線形回帰モデリングを実施する必要がある 

- 線形型のデータは限られている
- 非線形なデータに対して線形モデルだと説得力がない 
- 非線形な構造を捉える仕組みが必要

## 基底展開法 
非線形の場合、基底展開法という方法を使ってモデリングを行う 

- 回帰関数として、基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線形結合を利用
- 未知パラメータは、線形回帰モデルと同様に最小二乗法や最尤法による推定する。
- よく使われる基底関数 
    - 多項式関数 
    - ガウス型基底関数 
    - スプライン関数
 
```math
y_i=f(x_i)+\epsilon_i\qquad y_i=\omega_0+\sum_{i=1}^m\omega_j\phi_j(x_i)+\epsilon_i
```
$x$を線形の写像の$\phi$によって非線形化してから線形結合を見る 
ここで $\phi$が基底関数となる


### 多項式基底関数


```math
\phi_j=x^j
``` 
### ガウス型基底関数 

```math
\phi_j(x)=\exp\Biggl(\frac{(x-\mu_j)^2}{2h_j} \Biggr)
```


## モデル式
基底展開法も線形回帰と同じ枠組みで推定可能

説明変数:

```math
x_i=（x_{i1},x_{i2},\dots,x_{im})\in \mathbb{R}_m
```
非線形関数ベクトル:

```math
\phi(xi)=（\phi_1(x_i),\phi_2(x_i),\dots,\phi_k(x_i))^T∈\mathbb{R}^k
```
非線形関数の計画行列:

```math
\Phi_(train)=(\Phi(x_1),\Phi(x_2),\dots,\Phi(x_n))^T\in \mathbb{R}^{n×k}
```
最尤法による予測値:

```math
\hat{y}=\Phi(\Phi_(train)^T\Phi_(train))^{-1}\Phi_(train)^Ty_(train)
```
##未学習と過学習
### 未学習(underfitting)
学習データに対して、十分小さな誤差が得られないモデル 

対策 

- 表現力の高いモデルを利用する。


### 過学習（overfitting） 
小さな誤差は得られたが、テスト集合誤差との差が大きいモデル 
学習データに対して、小さな誤差が得られたが、検証データに対して、誤差が大きくなってしまう 

対策 

- 学習データの数を増やす 
- 不要な基底関数を削除して表現力を抑止 
- 正則化法を利用して表現力を抑止など

## 正則化法
モデルの複雑さに伴って、その値が大きくなる正則化項（罰則項）を課した関数を最小化

$S_γ=(y−\Phi w)^T(y−\Phi w)+γR(w)$  
正則化項 :$γR(w)$
形状によっていくつもの種類があり、それぞれ推定量の性質が異なる。

$γ$：正則化パラメータ 
モデルの曲線の滑らかさを調節 

## Ridge
正則化項にL2ノルムを使用したものをRidge推定 
パラメータを0に近づけるように推定
縮小推定と呼ばれる

## Lasso回帰
正則化項にL1ノルムを使用したものをLasso推定 
いくつかのパラメータを正確に0に推定
スパース推定と呼ばれる

## ホールドアウト法とクロスバリデーション法（交差検証）
### ホールドアウト法
データを学習用とテスト用の2つに分割し、予測精度や誤り率を推定するために使用 
手元に大量にデータがない場合、良い性能評価を与えないという欠点がある 
基底展開法に基づく非線形回帰モデルでは、基底関数の数、位置、チューニングを 
ホールドアウト値を小さくするモデルで決定する 

### クロスバリデーション法（交差検証）
データを学習用と検証用に分割し、検証用データを評価し精度の平均をCV値と呼ぶ
ホールドアウト法より、評価結果の信頼が高くなる。
## 非線形回帰演習   
真の関数は $y=1-48x+218x^2-315x^3+145x^4$ とした

真の関数にランダムにノイズを加えデータを生成
データから真の関数をそれぞれ線形回帰と非線形回帰によって予測する

## 演習結果 

線形回帰の精度：0.3901811689763751 
非線形回帰の精度：0.8824933990551088 
(基底関数はRBFで正規化強度は0.0002)  

## 考察
線形回帰は真の関数を表現しきれていない 
非線形回帰は真の関数を概ね表現できている 
非線形回帰の方が精度が良いことがわかる  
また正規化を行わない場合精度は9.9999になるが図から過学習して真の関数と遠くなる  

ラッソ回帰はパラメタをスパースにするので、確認してみるとすべて0なっている 

## ロジスティック回帰
