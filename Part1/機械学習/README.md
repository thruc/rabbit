# 機械学習
コンピュータプログラムは、タスクT(アプリケーションにさせたいこと)を性能指標Pで測定し、その性能が経験E(データ)により改善される場合、タスクTおよび性能指標Pに関して経験 Eから学習すると言われている

例）  株価の場合
タスクT→入力された過去の株価のデータを入力すると次の株価を予測する
性能指標P→予測した株価と実際の株価の差分
経験E→過去の株価のデータ  

## 回帰問題  
ある離散あるいは連続値の入力から連続値の出力で予測する問題  

- 直線で予測する場合→線形回帰問題
- 曲線で予測する場合→非線形回帰問題 

## 回帰問題で扱うデータ  
- 入力（各要素の説明変数または特徴量と呼ぶ)
    - m次元のベクトル 
- 出力（目的変数） 
    - スカラー値

説明変数:$x$ = ($x_1$ ,$x_2$ ,$\dots$,$x_m$)$^T$ $\in$ $\mathbb{R}^m$ 
目的変数:$y$=$\in$ $\mathbb{R}^m$ 。

例）住宅価格予測 
説明変数：部屋数、敷地面積や築年数 
目的変数：価格 

## 線形回帰 
- 回帰問題を解くための機械学習モデルのひとつ
- 教師あり学習　
- 入力とm次元のパラメータの線形結合を出力するモデル

線形結合(入力とパラメータの内積)

パラメータ： $w$ = ($w_1$ ,$w_2$ ,・・・,$w_m$)$^T$ $\subset$ $\mathbb{R}^m$ 
説明変数：$x$ = ($x_1$ ,$x_2$ ,・・・,$x_m$)$^T$ $\subset$ $\mathbb{R}^m$ 
予測値: $\hat{y}$  　
線形結合：

```math
\hat{y} = w^Tx + w_0= \sum_{j=1}^{m} w_jx_j + w_0 
```


## データ分割
モデルの汎化性能を測定するため、データを学習用データと検証用データに分割する 
モデルの汎化性能(Generalization)を測定するためデータへの当てはまりの良さあまり意味がない、未知のデータに対してどれくらいうまく予測できるかがしたいこと 

## 学習(最小二乗法)
### 平均二乗誤差 
データとモデル出力の二乗誤差

```math
MSE_{train} = \frac{1}{n_{train}}\sum_{i=1}^{n_{train}}(\hat{y}_i^{(train)}-y_i^{(train)})^2
```
 
### 最小二乗法 
- 学習データの平均二乗誤差を最小とするパラメータを探索する
- 学習データの平均二乗誤差を最小は、その勾配が０になる点を求めれば良い 

MSEを微分して0となるように解いていく$\hat{W}$(回帰係数)が求められる

```math
\hat{W} = (X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}
```
よって予測値$\hat{y}$は 

```math
\hat{y}=X\hat{W} = X(X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}
```
となる
## 線形回帰演習   
https://github.com/Tomo-Horiuchi/rabbit/blob/feature/ml/Part1/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/01_%E7%B7%9A%E5%9E%8B%E5%9B%9E%E5%B8%B0%E3%83%A2%E3%83%87%E3%83%AB/skl_regression.ipynb 
モデル評価法の参考
https://funatsu-lab.github.io/open-course-ware/basic-theory/accuracy-index/#r2
### 演習実施結果 
データを7対3で分けてそれぞれ分析し決定係数と平均二乗誤差を出しました
  
- 線形単回帰分析 

説明変数：部屋数
目的変数：価格

MSE Train : 44.983, Test : 40.412 
$R^2$ Train : 0.500, Test : 0.434  

- 重回帰分析(2変数) 
  
説明変数：部屋数、犯罪率 
目的変数：価格  
MSE Train : 40.586, Test : 34.377
$R^2$ Train : 0.549, Test : 0.518

### 考察
MSEがより0に近く,$R^2$の値が1に近いので重回帰分析の方が精度が良いことがわかる



## 非線形回帰 
複数な非線形構造を内在する現象に対して、非線形回帰モデリングを実施する必要がある 

- 線形型のデータは限られている
- 非線形なデータに対して線形モデルだと説得力がない 
- 非線形な構造を捉える仕組みが必要

## 基底展開法 
非線形の場合、基底展開法という方法を使ってモデリングを行う 

- 回帰関数として、基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線形結合を利用
- 未知パラメータは、線形回帰モデルと同様に最小二乗法や最尤法による推定する。
- よく使われる基底関数 
    - 多項式関数 
    - ガウス型基底関数 
    - スプライン関数
 
```math
y_i=f(x_i)+\epsilon_i\qquad y_i=\omega_0+\sum_{i=1}^m\omega_j\phi_j(x_i)+\epsilon_i
```
$x$を線形の写像の$\phi$によって非線形化してから線形結合を見る 
ここで $\phi$が基底関数となる


### 多項式基底関数


```math
\phi_j=x^j
``` 
### ガウス型基底関数 

```math
\phi_j(x)=\exp\Biggl(\frac{(x-\mu_j)^2}{2h_j} \Biggr)
```


## モデル式
基底展開法も線形回帰と同じ枠組みで推定可能

説明変数:

```math
x_i=（x_{i1},x_{i2},\dots,x_{im})\in \mathbb{R}_m
```
非線形関数ベクトル:

```math
\phi(xi)=（\phi_1(x_i),\phi_2(x_i),\dots,\phi_k(x_i))^T∈\mathbb{R}^k
```
非線形関数の計画行列:

```math
\Phi_(train)=(\Phi(x_1),\Phi(x_2),\dots,\Phi(x_n))^T\in \mathbb{R}^{n×k}
```
最尤法による予測値:

```math
\hat{y}=\Phi(\Phi_(train)^T\Phi_(train))^{-1}\Phi_(train)^Ty_(train)
```
##未学習と過学習
### 未学習(underfitting)
学習データに対して、十分小さな誤差が得られないモデル 

対策 

- 表現力の高いモデルを利用する。


### 過学習（overfitting） 
小さな誤差は得られたが、テスト集合誤差との差が大きいモデル 
学習データに対して、小さな誤差が得られたが、検証データに対して、誤差が大きくなってしまう 

対策 

- 学習データの数を増やす 
- 不要な基底関数を削除して表現力を抑止 
- 正則化法を利用して表現力を抑止など

## 正則化法
モデルの複雑さに伴って、その値が大きくなる正則化項（罰則項）を課した関数を最小化

$S_γ=(y−\Phi w)^T(y−\Phi w)+γR(w)$  
正則化項 :$γR(w)$
形状によっていくつもの種類があり、それぞれ推定量の性質が異なる。

$γ$：正則化パラメータ 
モデルの曲線の滑らかさを調節 

## Ridge
正則化項にL2ノルムを使用したものをRidge推定 
パラメータを0に近づけるように推定
縮小推定と呼ばれる

## Lasso回帰
正則化項にL1ノルムを使用したものをLasso推定 
いくつかのパラメータを正確に0に推定
スパース推定と呼ばれる

## ホールドアウト法とクロスバリデーション法（交差検証）
### ホールドアウト法
データを学習用とテスト用の2つに分割し、予測精度や誤り率を推定するために使用 
手元に大量にデータがない場合、良い性能評価を与えないという欠点がある 
基底展開法に基づく非線形回帰モデルでは、基底関数の数、位置、チューニングを 
ホールドアウト値を小さくするモデルで決定する 

### クロスバリデーション法（交差検証）
データを学習用と検証用に分割し、検証用データを評価し精度の平均をCV値と呼ぶ
ホールドアウト法より、評価結果の信頼が高くなる。
## 非線形回帰演習   
真の関数は $y=1-48x+218x^2-315x^3+145x^4$ とした

真の関数にランダムにノイズを加えデータを生成
データから真の関数をそれぞれ線形回帰と非線形回帰によって予測する

## 演習結果 

線形回帰の精度：0.3901811689763751 
非線形回帰の精度：0.8824933990551088 
(基底関数はRBFで正規化強度は0.0002)  

## 考察
線形回帰は真の関数を表現しきれていない 
非線形回帰は真の関数を概ね表現できている 
非線形回帰の方が精度が良いことがわかる  
また正規化を行わない場合精度は9.9999になるが図から過学習して真の関数と遠くなる  

ラッソ回帰はパラメタをスパースにするので、確認してみるとすべて0なっている 

## ロジスティック回帰
ロジスティック回帰モデルは分類である
分類問題とはある入力からクラスを分類する問題   
分類で扱うデータ
入力：$x=(x_1,x2_2,\dots,x_m)^T\in \mathbb{R}^m$（m次元のベクトル）
出力：$y\in \Bigl\\{0,1\Bigr\\} $（0か1の値）
例） 
タイタニックのデータ、IRISデータ  
このようなデータをそのまま回帰モデルで当てはめてしまうと確立として意味を持たない値になってしまう 
なのでロジスティック回帰モデルは入力とm次元パラメータの線形結合をシグモイド関数に入力にする
## シグモイド関数
入力が実数のとき必ず出力が0～1になるような単調増加関数  


シグモイド関数$\sigma(x)$  

$$
\sigma(x)=\frac{1}{1+\exp{(-ax)}}
$$ 
特徴 

- $a$を増加させると$x=0$付近で曲線の勾配が増加
- $a$を極めて大きくすると単位ステップ関数に近づく
- バイアス変化は段差の位置  

## シグモイド関数の性質　
- 微分が自身で表現できることが可能
- 尤度関数の計算が楽になる  

シグモイド関数の微分 

$$
\frac{\vartheta\sigma(x)}{\vartheta x} = a\sigma(x)(1-\sigma(x))
$$
となるので自身の関数で表現できている


## 定式化
求めたい値（$Y=1$になる確率）

$$
P(Y=1|x)=\sigma(\omega_0+\omega_1 x_1 + \dots + \omega_m x_m)
$$
と表記できる
データ$Y$は確率が0.50以上であれば分類は 1、未満ならば分類は0とする 
この数式をどのように考えればよいかというときに最尤推定法を使用する
## 最尤推定
ロジスティック回帰モデルではベルヌーイ分布を利用する 
ある分布を考えた際、そのパラメータによって生成されたるデータは変化し 
そのデータからそのデータを生成したであろう尤もらしい分布を推定する方法が最尤推定 
尤度関数を最大化するようなパラメータを選ぶ方法を最尤推定という
### 尤度関数
データを固定しパラメータを変化させる
1回の試行で$y=y_1$となる確率

$$
P(y)=p^y(1-p)^{1-y}
$$
n回の試行で同時に$y_1～y_n$が起こる確率（p固定）

$$
P(y_1,y_2,\dots,y_n;p)= \prod_{i=1}^np^{y_i}(1-p)^{1-y_i}
$$
となる、ここで与えられたデータyを固定しpを変数として推定する 
pが最大になるとき最もらしくなる 
なのでpに対して最適化問題を解けばよい  


### ロジスティック回帰モデルの最尤推定  
  


- 確率 $p$　はシグモイド関数となるため推定するパラメータ確認は重みのパラメータとなる
- 目的変数と説明変数を生成するもっともらしいパラメータを探す



  
$$
P(Y=y_n|x_n)=p^{y_n}(1-p_n)^{1-y_n} = \sigma(w^Tx_n)^{y_n}(1-\sigma(w^Tx_n))^{1-y_n}=L(w_n)
$$

         
$w$が未知 

- 掛け算が多いので対数を取ると微分の計算が楽
- 対数尤度関数が最大になる点と尤度関数が最大にする点は同じ
- 尤度関数にマイナスをかけたものを最小化２乗法の最小化と合わせる最小化問題に変換しする

よって上記の$L(w)$対数を取ってマイナスをかけたものを最小化問題でとけばいいい

$$
E(w_0,w_1.w_2,\dots,w_n)=-logL(w_0,w_1,w_2.\dots,w_n)
$$


## 勾配降下法
ロジスティック回帰の場合、尤度関数が最小となるパラメータを求めることができない。
そのため、勾配降下法によって逐次的にパラメータを更新する。
ただし、この勾配降下法をそのまま利用すると、一回のパラメータ更新に関して、すべての入力データが必要である、というデメリットがある。
入力データが巨大になったとき、計算時間・メモリ不足などが問題となる。
この点を解決するために、確率的勾配降下法というものが存在する。
反復学習によりパラメータを逐次的に更新する
アプローチのひとつ。学習率でパラメータの収束しやすさを調整。ロジスティック回帰
ではなく、対数尤度関数をパラメータで微分して0になる値を求める必要があるが、解析的に求めることは困難であるため勾配降下法を用いる。パラメータ
が更新されなくなると勾配が0になったことを意味し、反復学習の範囲では最適な解が求められたことになる。

## 確率的勾配法
パラメータに初期値を与え、徐々にパラメータを更新し、収束した時点でそのときのパラメータを最適値として採用する。
学習率  η  は、パラメータ更新の「歩幅」を表す。
小さいと収束までに時間を要する。
大きすぎると最適値を「飛び越えて」しまう（本当に一番求めたい点を求めるのが難しくなる）ような事象が発生する。
## モデルの評価
【混同行列】 



|     | 検証データ positive    |    検証用データ negative | 
| --- | --- | --- | 
| 予想結果 positive     |   真陽性（Ture Positive）  | 偽陰性（False　Positive）    | 
| 予想結果 negative   | 偽陽性（False　Negative）    |  真陰性（Ture Negative）   | 




【適合率と再現率、F値】

正解率

$$
\frac{TP+TN}{TP+TN+FP+FN)}
$$


再現率(recall)

$$
\frac{TP}{TP+FN}
$$ 
適合率 (prescision)
$$
\frac{TP}{TP+FP}
$$
F値(再現率と適合率の調和平均)
$$
2×適合率×再現率適合率+再現率
$$
recallはprecisionトレードオフの関係 

ハンズオン
【演習実施結果】


【考察】

ロジスティック回帰はパラメトリックな手法であるため、タイタニックデータの扱いに関しても、厳密には予測の入力となるデータに関しても、分布状況の確認・検定が必要なのかもしれないと考えた。(例えば「クラス」)
客室番号は使えるのだろうか、と考えていたが、もしかすると同一の客室である場合は同一の結果(死亡 or 生存)となる傾向などがあるのではないかと考えた。
正解や適合率は母集団の分布に依存するが再現率に関しては依存しない。そういった意味で、母集団の分布に依存せずに TN の結果を評価する「特異度」  TNTN+FP  が有用な場合もあるのではないかと考えた。(AI領域では一般的ではない？)
