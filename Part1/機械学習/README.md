# 機械学習
コンピュータプログラムは、タスクT(アプリケーションにさせたいこと)を性能指標Pで測定し、その性能が経験E(データ)により改善される場合、タスクTおよび性能指標Pに関して経験 Eから学習すると言われている

例）  株価の場合
タスクT→入力された過去の株価のデータを入力すると次の株価を予測する
性能指標P→予測した株価と実際の株価の差分
経験E→過去の株価のデータ  


## 回帰問題  
ある離散あるいは連続値の入力から連続値の出力で予測する問題  

- 直線で予測する場合→線形回帰問題
- 曲線で予測する場合→非線形回帰問題 

## 回帰問題で扱うデータ  
- 入力（各要素の説明変数または特徴量と呼ぶ)
    - m次元のベクトル 
- 出力（目的変数） 
    - スカラー値

説明変数:$x$ = ($x_1$ ,$x_2$ ,$\dots$,$x_m$)$^T$ $\in$ $\mathbb{R}^m$ 
目的変数:$y$=$\in$ $\mathbb{R}^m$ 。

例）住宅価格予測 
説明変数：部屋数、敷地面積や築年数 
目的変数：価格 

## 線形回帰 
- 回帰問題を解くための機械学習モデルのひとつ
- 教師あり学習　
- 入力とm次元のパラメータの線形結合を出力するモデル

線形結合(入力とパラメータの内積)

パラメータ： $w$ = ($w_1$ ,$w_2$ ,・・・,$w_m$)$^T$ $\subset$ $\mathbb{R}^m$ 
説明変数：$x$ = ($x_1$ ,$x_2$ ,・・・,$x_m$)$^T$ $\subset$ $\mathbb{R}^m$ 
予測値: $\hat{y}$  　
線形結合：

```math
\hat{y} = w^Tx + w_0= \sum_{j=1}^{m} w_jx_j + w_0 
```


## データ分割
モデルの汎化性能を測定するため、データを学習用データと検証用データに分割する 
モデルの汎化性能(Generalization)を測定するためデータへの当てはまりの良さあまり意味がない、未知のデータに対してどれくらいうまく予測できるかがしたいこと 

## 学習(最小二乗法)
### 平均二乗誤差 
データとモデル出力の二乗誤差

```math
MSE_{train} = \frac{1}{n_{train}}\sum_{i=1}^{n_{train}}(\hat{y}_i^{(train)}-y_i^{(train)})^2
```
 
### 最小二乗法 
- 学習データの平均二乗誤差を最小とするパラメータを探索する
- 学習データの平均二乗誤差を最小は、その勾配が０になる点を求めれば良い 

MSEを微分して0となるように解いていく$\hat{W}$(回帰係数)が求められる

```math
\hat{W} = (X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}
```
よって予測値$\hat{y}$は 

```math
\hat{y}=X\hat{W} = X(X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}
```
となる
## 線形回帰演習
演習実施結果