# Section1 強化学習
## 強化学習とは
長期的に報酬を最大化できるように環境の中で行動を選択できるエージェントを作ることを目標とする機械学習の一分野
→　行動の結果として与えられる利益（報酬）を基に、行動を決定する原理を改善していく仕組み

![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/457374/3763feab-ad26-d4b5-a0b8-34b7d9ae5db9.png)

エージェント：主人公

エージェントが方針に基づいて行動しそれに見合う環境から報酬がもらえる
報酬が最大になるように方針を訓練していくイメージ

## 強化学習の応用例
マーケティングの場合
エージェント:プロフィールと購入履歴に基づいて、キャンペーンメールを送る顧客を決めるソフトウェアである。行動:顧客ごとに送信、非送信のふたつの行動を選ぶことになる。報酬:キャンペーンのコストという負の報酬とキャンペーンで生み出されると推測される売上という正の報酬を受ける

## 探索と利用のトレードオフ
利用が足りない状態⇔探索が足りない状態がトレードオフの関係
強化学習ではこれをうまく調整していく
### 探索が足りない状態
過去のデータでベストとされる行動のみを常にとり続ければ、他のさらにベストな行動を見つけることはできない
### 利用が足りない状態
未知の行動のみを常にとり続ければ、過去の経験が活かせない

## 強化学習の差分
強化学習と通常の教師あり、教師なし学習との違い
目標が違う ・教師なし、あり学習では、データに含まれるパターンを見つけ出す およびそのデータから予測することが目標
強化学習では、優れた方策を見つけることが目標

## 価値関数
価値を表す関数としては、状態価値関数と行動価値関数の2種類がある

### 状態価値関数
価値を決める際環境の状態の価値に注目する場合
環境の状態が良ければ価値が上がる
エージェントの行動は関係ない

```math
V^{\pi}(s)
```

### 行動価値関数
価値を決める際環境の状態と価値を組み合わせた価値に注目する場合
エージェントがある状態で行動したときの価値

```math
Q^{\pi}(s,a)
```

## 方策関数
ある環境の状態においてどのような行動をとるのか確率を与える関数

```math
\pi(s)=a
```



## 方策勾配法
方策をモデルにすることで最適化する手法

```math
\theta^{(t+1)}=\theta^{(t)}+\epsilon\nabla J(\theta)
```

```math
\nabla_{\theta}J(\theta)=\mathbb{E}_{\pi_{\theta}}[(\nabla_{\theta}log\pi_{\theta}(a|s)Q^{\pi}(s,a))]

```
$t$：時間
$\theta$：重み
$\epsilon$：学習率
$J$：誤差関数

# Section2 Alpha Go
AlphaGo LeeとAlphaGo Zero二種類ある

## AlphaGo Lee
ValueNetとPolicyNetのCNNを利用している



### PolicyNet(方策関数)
19x19の2次元データを利用
48チャンネル持っている
19x19の着手予想確率得られる

![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/457374/7bb5fd26-10d9-7f14-bf1b-048dbc02763a.png)


### ValueNet(価値関数)
19x19の2次元データを利用
49チャンネル持っている（手番が追加）
勝率を-1～1の範囲で得られる
勝つか負けるかの出力であるためFlattenを挟んである

![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/457374/d51a8315-2c46-3970-9576-33f1da946a28.png)




### Alpha Go Leeの学習ステップ
1．教師あり学習でRollOutPolicyとPolicyNetの学習
2．強化学習でPolicyNetの学習
3．強化学習でValueNetの学習

#### RollOutPolicy
NNではなく線形の方策関数
探索中に高速に着手確率を出すために使用される。

### モンテカルロ木探索
コンピューター囲碁ソフトで現在もっとも有効とされている探索法

## AlphaGo Zero

### AlphaGo LeeとAlphaGo Zeroの違い

1.教師あり学習を一切行わず、強化学習のみで作成
2.特徴入力からヒューリスティックな要素を排除し、石の配置のみにした
3.PolicyNetとValueNetを１つのネットワークに統合した
4.Residual Net（後述）を導入した５、モンテカルロ木探索からRollOutシミュレーションをなくした


### PolicyValueNet
![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/457374/dd892db8-1432-ce7f-9d97-904138aad91e.png)

PolicyNetとValueNetが統合し、それぞれ方策関数と価値観数の出力が得たいため途中で枝分かれした構造のNNとなる。


### Residual Block
ネットワークにショートカットを作る
ネットワークの深さを抑える
勾配消失問題が起きにくくなる
基本構造はConvolution→BatchNorm→ReLU→Convolution→BatchNorm→Add→ReLU
アンサンブル効果


### PreActivation
Residual Blockの並びをBatchNor→ReLU→Convolution→BatchNorm→ReLU→Convolution→Addにして性能向上

### wideResNet
Convolutionのフィルタをk倍にしたResNet。
フィルタを増やすことで層が浅くても深い層のものと同等以上の性能を発揮

### PyramidNet
各層でフィルタ数を増やしていくResNet

# aection3 軽量化・高速化技術
どうやってモデルを高速に学習するか
どうやって高性能ではないコンピューターでモデル動かすか
## 分散深層学習
- 深層学習は多くのデータを使用したり、パラメータ調整のために多くの時間を使用したりするため、高速な計算が求められる
- 複数の計算資源(ワーカー)を使用し、並列的にニューラルネットを構成することで、効率の良い学習を行いたい
- データ並列化、モデル並列化、GPUによる高速技術は不可欠である

## データ並列
- 親モデルを各ワーカー(コンピューターなど)に子モデルとしてコピー
- データを分割し、各ワーカーごとに計算させる

コンピューター自体やGPUやTPUなどを増やし計算を分散し学習を高速にする
データ並列化は各モデルのパラメータの合わせ方で、同期型か非同期型か決まる

### 同期型

同期型のパラメータ更新の流れ。各ワーカーが計算が終わるのを待ち、全ワーカーの勾配が出たところで勾配の平均を計算し、親モデルのパラメータを更新する。


### 非同期型
各ワーカーはお互いの計算を待たず、各子モデルごとに更新を行う。学習が終わった子モデルはパラメータサーバにPushされる。新たに学習を始める時は、パラメータサーバからPopしたモデルに対して学習していく

### 同期・非同期の比較
- 処理のスピードは、お互いのワーカーの計算を待たない非同期型の方が早い
- 非同期型は最新のモデルのパラメータを利用できないので、学習が不安定になりやすい
-> Stale Gradient Problem
- 現在は同期型の方が精度が良いことが多いので、主流となっている。

## モデル並列
- 親モデルを各ワーカーに分割し、それぞれのモデルを学習させる。全てのデータで学習が終わった後で、一つのモデルに復元
- モデルが大きい時はモデル並列化を、データが大きい時はデータ並列化をすると良い

モデルのパラメータが多い場合ほど、効率化も向上する

## GPUによる高速化

### GPGPU (General-purpose on GPU)
元々の使用目的であるグラフィック以外の用途で使用されるGPUの総称
#### CPU
高性能なコアが少数
複雑で連続的な処理が得意
#### GPU
比較的低性能なコアが多数
簡単な並列処理が得意
ニューラルネットの学習は単純な行列演算が多いので、高速化が可能

### GPGPUの開発環境
#### CUDA 
GPU上で並列コンピューティングを行うためのプラットフォーム
NVIDIA社が開発しているGPUのみで使用可能
Deep Learning用に提供されているので、使いやすい
#### OpenCL
オープンな並列コンピューティングのプラットフォーム
NVIDIA社以外の会社(Intel, AMD, ARMなど)のGPUからでも使用可能
Deep Learning用の計算に特化しているわけではない


## 軽量化
- 量子化
- 蒸留
- プルーニング

量子化はよく使用されている

## 量子化
ネットワークが大きくなると大量のパラメータが必要なり学習や推論に多くのメモリと演算処理が必要
→通常のパラメータの64 bit 浮動小数点を32 bit など下位の精度に落とすことでメモリと演算処理の削減を行う

数十億個のパラメータがあると重みを記憶するために多くのメモリが必要
パラメータ一つの情報の精度を落とし記憶する情報量を減らしていく
倍精度演算(64 bit)と単精度演算(32 bit)は演算性能が大きく違うため、量子化により精度を落とすことによりより多くの計算をすることができる。
16bitが無難


### メリット
計算の高速化
省メモリ化

### デメリット
精度の低下

## 蒸留
精度の高いモデルはニューロンの規模が大きなモデルとなっていてそのため、推論に多くのメモリと演算処理が必要
→規模の大きなモデルの知識を使い軽量なモデルの作成を行う

### モデルの簡約化
学習済みの精度の高いモデルの知識を軽量なモデルに継承させる

### メリット
蒸留によって少ない学習回数でより精度の良いモデルを作成することができる


## プルーニング
ネットワークが大きくなると大量のパラメータがすべてのニューロンが計算の精度に関係しているわけではない
→モデルの精度に寄与が少ないニューロンを削除することでモデルの軽量化・高速化する

### ニューロン数と精度
精度にどのくらい寄与しているかの閾値を決めニューロン削除するものをきめる
閾値が高くすることによりニューロン数が減少し精度が減少する





## DenseNet
## BatchNorm
## Layer Norm
## Wavenet







