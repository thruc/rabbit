{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "def get_data_url(data_nums, pid, rno):\n",
    "    base = 'hoge'\n",
    "    baseAdd1 = \"/fuga/\"\n",
    "    baseAdd2 = \"/piyo/\" + pid + \"/hogera/\" + rno + \"/\"\n",
    "    url = base + baseAdd1 + data_nums + baseAdd2\n",
    "    return urllib.parse.quote_plus(url, \"/:?=&\")\n",
    "\n",
    "def date_span(start_date, end_date):\n",
    "    \"\"\"start_date、end_dateの期間に含まれる日毎のdatetimeオブジェクトを返すジェネレータ\n",
    "    \"\"\"\n",
    "    for i in range((end_date - start_date).days + 1):\n",
    "        yield start_date + timedelta(i)\n",
    "\n",
    "start_date = datetime.strptime('20170822', '%Y%m%d')\n",
    "end_date = datetime.strptime('20170823', '%Y%m%d')\n",
    "\n",
    "today = datetime.today()\n",
    "date_prefix = today.strftime('%Y-%m-%d %H:%M:%S')\n",
    "file_origin = 'crawl_' + date_prefix + '_Crawler.csv'\n",
    "\n",
    "my_file = Path(file_origin)\n",
    "isFirst = True\n",
    "\n",
    "if my_file.is_file():\n",
    "    mainDf = pd.read_csv(file_origin, index_col=0, header=0)\n",
    "    isFirst = False\n",
    "else:\n",
    "    mainDf = None\n",
    "    \n",
    "place_ids = []\n",
    "race_noms = []\n",
    "for i in range(1, 31):\n",
    "    conv_num = str(i).zfill(2)\n",
    "    place_ids.append(conv_num)\n",
    "    if i <= 12:\n",
    "        race_noms.append(conv_num) \n",
    "\n",
    "sleep_time = 5\n",
    "\n",
    "for target_date in date_span(start_date, end_date):\n",
    "    for pid in place_ids:\n",
    "        for rno in race_noms:\n",
    "            target_url = get_data_url(target_date.strftime('%Y%m%d'), pid, rno)\n",
    "            date = target_date.strftime('%s')\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            time.sleep(sleep_time)\n",
    "            response = requests.get(target_url, headers=headers)# <Response [200]>\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            content1 = soup.find_all(\"h1\", class_=\"h_content1\")\n",
    "            content2 = soup.find_all(\"div\", class_=\"blocks\")\n",
    "            content3 = soup.find_all(\"table\", id=\"detail_program\")\n",
    "            for content3_part in content3:\n",
    "                tds1 = [td.find_all(\"p\") for td in content3_part.find_all(\"td\", class_=\"border_all name_kanji\")]\n",
    "                ID = [row[0].string for row in tds1]\n",
    "                name = [row[1].string for row in tds1]\n",
    "                prefecture = [row[2].string.split(\"/\") for row in tds1]\n",
    "                age = [row[3].string.split(\"/\") for row in tds1]\n",
    "                ages = [row[0].replace('歳', '') for row in age]\n",
    "                term = [row[1].replace('期', '') for row in age]\n",
    "                prefectures = [row[0].replace('北海道', '1').replace('青森', '2').replace('岩手', '3').replace('宮城', '4').replace('秋田', '5').replace('山形', '6').replace('福島', '7').replace('茨城', '8').replace('栃木', '9').replace('群馬', '10').replace('埼玉', '11').replace('千葉', '12').replace('東京', '13').replace('神奈川', '14').replace('新潟', '15').replace('富山', '16').replace('石川', '17').replace('福井', '18').replace('山梨', '19').replace('長野', '20').replace('岐阜', '21').replace('静岡', '22').replace('愛知', '23').replace('三重', '24').replace('滋賀', '25').replace('京都', '26').replace('大阪', '27').replace('兵庫', '28').replace('奈良', '29').replace('和歌山', '30').replace('鳥取', '31').replace('島根', '32').replace('岡山', '33').replace('広島', '34').replace('山口', '35').replace('徳島', '36').replace('香川', '37').replace('愛媛', '38').replace('高知', '39').replace('福岡', '40').replace('佐賀', '41').replace('長崎', '42').replace('熊本', '43').replace('大分', '44').replace('宮崎', '45').replace('鹿児島', '46').replace('沖縄', '47') for row in prefecture]\n",
    "                rank = [row[1].replace('A1', '1').replace('A2', '2').replace('B1', '3').replace('B2', '4') for row in prefecture]\n",
    "                tds2 = [td.text for td in content3_part.find_all(\"td\", class_=\"border_all average\")[2:][::5]]\n",
    "                morter_num = [td[:-5] for td in tds2]\n",
    "                for content2_part in content2:\n",
    "                    race_number = [p.text.replace('【', '').replace('】', '').replace('予選', '').replace('R', '') for p in content2_part.find_all(\"p\", class_=\"left\")]\n",
    "                    for num in race_number:\n",
    "                        race_number = [num for i in range(6)]\n",
    "                        race_number = [num[:3] for num in race_number]\n",
    "                for content1_part in content1:\n",
    "                    race_place = [n[:3] for n in content1_part]\n",
    "                    race_place = race_place * 6\n",
    "                raceDict = pd.DataFrame({\"date\": date,                                         \n",
    "                                        \"ID\": ID,\n",
    "                                        \"name\": name,\n",
    "                                        \"age\": ages,\n",
    "                                        \"term\": term,\n",
    "                                        \"prefecture\": prefectures,\n",
    "                                        \"rank\": rank,\n",
    "                                        \"morter_num\": morter_num,\n",
    "                                        \"race_number\": race_number,\n",
    "                                        \"race_place\": race_place\n",
    "                                         })\n",
    "\n",
    "                # 取得した後、追加していく\n",
    "                if mainDf is None:\n",
    "                    raceDict.to_csv(file_origin)\n",
    "                    mainDf = raceDict\n",
    "                else:\n",
    "                    raceDict.to_csv(file_origin, mode='a', header=False)\n",
    "                    mainDf = mainDf.append(raceDict)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
