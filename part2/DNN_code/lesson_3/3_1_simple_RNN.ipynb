{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"3_1_simple_RNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"8cNl2QA_Rnv5","colab_type":"text"},"source":["# 準備"]},{"cell_type":"markdown","metadata":{"id":"YkwjN1jNVAYy","colab_type":"text"},"source":["## Googleドライブのマウント"]},{"cell_type":"code","metadata":{"id":"pvFXpiH3EVC1","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ub7RYdeY6pK","colab_type":"text"},"source":["## sys.pathの設定"]},{"cell_type":"markdown","metadata":{"id":"oql7L19rEsWi","colab_type":"text"},"source":["以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"]},{"cell_type":"code","metadata":{"id":"7Ic2JzkvFX59","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/drive/My Drive/DNN_code')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"feXB1SiLP4OL","colab_type":"text"},"source":["# simple RNN\n","### バイナリ加算"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"tzSWNYwxP4OM","colab_type":"code","colab":{}},"source":["import numpy as np\n","from common import functions\n","import matplotlib.pyplot as plt\n","\n","# def d_tanh(x):\n","\n","\n","\n","# データを用意\n","# 2進数の桁数\n","binary_dim = 8\n","# 最大値 + 1\n","largest_number = pow(2, binary_dim)\n","# largest_numberまで2進数を用意\n","binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n","\n","input_layer_size = 2\n","hidden_layer_size = 16\n","output_layer_size = 1\n","\n","weight_init_std = 1\n","learning_rate = 0.1\n","\n","iters_num = 10000\n","plot_interval = 100\n","\n","# ウェイト初期化 (バイアスは簡単のため省略)\n","W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n","W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n","W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n","\n","# Xavier\n","\n","\n","# He\n","\n","\n","\n","# 勾配\n","W_in_grad = np.zeros_like(W_in)\n","W_out_grad = np.zeros_like(W_out)\n","W_grad = np.zeros_like(W)\n","\n","u = np.zeros((hidden_layer_size, binary_dim + 1))\n","z = np.zeros((hidden_layer_size, binary_dim + 1))\n","y = np.zeros((output_layer_size, binary_dim))\n","\n","delta_out = np.zeros((output_layer_size, binary_dim))\n","delta = np.zeros((hidden_layer_size, binary_dim + 1))\n","\n","all_losses = []\n","\n","for i in range(iters_num):\n","    \n","    # A, B初期化 (a + b = d)\n","    a_int = np.random.randint(largest_number/2)\n","    a_bin = binary[a_int] # binary encoding\n","    b_int = np.random.randint(largest_number/2)\n","    b_bin = binary[b_int] # binary encoding\n","    \n","    # 正解データ\n","    d_int = a_int + b_int\n","    d_bin = binary[d_int]\n","    \n","    # 出力バイナリ\n","    out_bin = np.zeros_like(d_bin)\n","    \n","    # 時系列全体の誤差\n","    all_loss = 0    \n","    \n","    # 時系列ループ\n","    for t in range(binary_dim):\n","        # 入力値\n","        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n","        # 時刻tにおける正解データ\n","        dd = np.array([d_bin[binary_dim - t - 1]])\n","        \n","        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n","        z[:,t+1] = functions.sigmoid(u[:,t+1])\n","\n","        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n","\n","\n","        #誤差\n","        loss = functions.mean_squared_error(dd, y[:,t])\n","        \n","        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n","        \n","        all_loss += loss\n","\n","        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n","    \n","    \n","    for t in range(binary_dim)[::-1]:\n","        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n","\n","        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n","\n","        # 勾配更新\n","        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n","        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n","        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n","    \n","    # 勾配適用\n","    W_in -= learning_rate * W_in_grad\n","    W_out -= learning_rate * W_out_grad\n","    W -= learning_rate * W_grad\n","    \n","    W_in_grad *= 0\n","    W_out_grad *= 0\n","    W_grad *= 0\n","    \n","\n","    if(i % plot_interval == 0):\n","        all_losses.append(all_loss)        \n","        print(\"iters:\" + str(i))\n","        print(\"Loss:\" + str(all_loss))\n","        print(\"Pred:\" + str(out_bin))\n","        print(\"True:\" + str(d_bin))\n","        out_int = 0\n","        for index,x in enumerate(reversed(out_bin)):\n","            out_int += x * pow(2, index)\n","        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n","        print(\"------------\")\n","\n","lists = range(0, iters_num, plot_interval)\n","plt.plot(lists, all_losses, label=\"loss\")\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O7zQEPrtP4OP","colab_type":"text"},"source":["---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n","\n","\n","## [try] 重みの初期化方法を変更してみよう\n","Xavier, He\n","\n","## [try] 中間層の活性化関数を変更してみよう\n","ReLU(勾配爆発を確認しよう)<br>\n","tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n","\n","---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"]}]}