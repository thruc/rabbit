{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"name":"predict_word.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8cNl2QA_Rnv5","colab_type":"text"},"source":["# 準備"]},{"cell_type":"markdown","metadata":{"id":"YkwjN1jNVAYy","colab_type":"text"},"source":["## Googleドライブのマウント"]},{"cell_type":"code","metadata":{"id":"pvFXpiH3EVC1","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ub7RYdeY6pK","colab_type":"text"},"source":["## ディレクトリを移動"]},{"cell_type":"markdown","metadata":{"id":"oql7L19rEsWi","colab_type":"text"},"source":["以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"]},{"cell_type":"code","metadata":{"id":"_2oaYEj_8fsY","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('drive/My Drive/DNN_code/lesson_3/3_2_tf_languagemodel/')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tmFoITp02_Gv","colab_type":"text"},"source":["# predict_word"]},{"cell_type":"code","metadata":{"id":"d9_Wdzdj2Sj-","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import re\n","import glob\n","import collections\n","import random\n","import pickle\n","import time\n","import datetime\n","import os\n","\n","# logging levelを変更\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","class Corpus:\n","    def __init__(self):\n","        self.unknown_word_symbol = \"<???>\" # 出現回数の少ない単語は未知語として定義しておく\n","        self.unknown_word_threshold = 3 # 未知語と定義する単語の出現回数の閾値\n","        self.corpus_file = \"./corpus/**/*.txt\"\n","        self.corpus_encoding = \"utf-8\"\n","        self.dictionary_filename = \"./data_for_predict/word_dict.dic\"\n","        self.chunk_size = 5\n","        self.load_dict()\n","\n","        words = []\n","        for filename in glob.glob(self.corpus_file, recursive=True):\n","            with open(filename, \"r\", encoding=self.corpus_encoding) as f:\n","\n","                # word breaking\n","                text = f.read()\n","                # 全ての文字を小文字に統一し、改行をスペースに変換\n","                text = text.lower().replace(\"\\n\", \" \")\n","                # 特定の文字以外の文字を空文字に置換する\n","                text = re.sub(r\"[^a-z '\\-]\", \"\", text)\n","                # 複数のスペースはスペース一文字に変換\n","                text = re.sub(r\"[ ]+\", \" \", text)\n","\n","                # 前処理： '-' で始まる単語は無視する\n","                words = [ word for word in text.split() if not word.startswith(\"-\")]\n","\n","\n","        self.data_n = len(words) - self.chunk_size\n","        self.data = self.seq_to_matrix(words)\n","\n","    def prepare_data(self):\n","        \"\"\"\n","        訓練データとテストデータを準備する。\n","        data_n = ( text データの総単語数 ) - chunk_size\n","        input: (data_n, chunk_size, vocabulary_size)\n","        output: (data_n, vocabulary_size)\n","        \"\"\"\n","\n","        # 入力と出力の次元テンソルを準備\n","        all_input = np.zeros([self.chunk_size, self.vocabulary_size, self.data_n])\n","        all_output = np.zeros([self.vocabulary_size, self.data_n])\n","\n","        # 準備したテンソルに、コーパスの one-hot 表現(self.data) のデータを埋めていく\n","        # i 番目から ( i + chunk_size - 1 ) 番目までの単語が１組の入力となる\n","        # このときの出力は ( i + chunk_size ) 番目の単語\n","        for i in range(self.data_n):\n","            all_output[:, i] = self.data[:, i + self.chunk_size] # (i + chunk_size) 番目の単語の one-hot ベクトル\n","            for j in range(self.chunk_size):\n","                all_input[j, :, i] = self.data[:, i + self.chunk_size - j - 1]\n","\n","        # 後に使うデータ形式に合わせるために転置を取る\n","        all_input = all_input.transpose([2, 0, 1])\n","        all_output = all_output.transpose()\n","\n","        # 訓練データ：テストデータを 4 : 1 に分割する\n","        training_num = ( self.data_n * 4 ) // 5\n","        return all_input[:training_num], all_output[:training_num], all_input[training_num:], all_output[training_num:]\n","\n","\n","    def build_dict(self):\n","        # コーパス全体を見て、単語の出現回数をカウントする\n","        counter = collections.Counter()\n","        for filename in glob.glob(self.corpus_file, recursive=True):\n","            with open(filename, \"r\", encoding=self.corpus_encoding) as f:\n","\n","                # word breaking\n","                text = f.read()\n","                # 全ての文字を小文字に統一し、改行をスペースに変換\n","                text = text.lower().replace(\"\\n\", \" \")\n","                # 特定の文字以外の文字を空文字に置換する\n","                text = re.sub(r\"[^a-z '\\-]\", \"\", text)\n","                # 複数のスペースはスペース一文字に変換\n","                text = re.sub(r\"[ ]+\", \" \", text)\n","\n","                # 前処理： '-' で始まる単語は無視する\n","                words = [word for word in text.split() if not word.startswith(\"-\")]\n","\n","                counter.update(words)\n","\n","        # 出現頻度の低い単語を一つの記号にまとめる\n","        word_id = 0\n","        dictionary = {}\n","        for word, count in counter.items():\n","            if count <= self.unknown_word_threshold:\n","                continue\n","\n","            dictionary[word] = word_id\n","            word_id += 1\n","        dictionary[self.unknown_word_symbol] = word_id\n","\n","        print(\"総単語数：\", len(dictionary))\n","\n","        # 辞書を pickle を使って保存しておく\n","        with open(self.dictionary_filename, \"wb\") as f:\n","            pickle.dump(dictionary, f)\n","            print(\"Dictionary is saved to\", self.dictionary_filename)\n","\n","        self.dictionary = dictionary\n","\n","        print(self.dictionary)\n","\n","    def load_dict(self):\n","        with open(self.dictionary_filename, \"rb\") as f:\n","            self.dictionary = pickle.load(f)\n","            self.vocabulary_size = len(self.dictionary)\n","            self.input_layer_size = len(self.dictionary)\n","            self.output_layer_size = len(self.dictionary)\n","            print(\"総単語数: \", self.input_layer_size)\n","\n","    def get_word_id(self, word):\n","        # print(word)\n","        # print(self.dictionary)\n","        # print(self.unknown_word_symbol)\n","        # print(self.dictionary[self.unknown_word_symbol])\n","        # print(self.dictionary.get(word, self.dictionary[self.unknown_word_symbol]))\n","        return self.dictionary.get(word, self.dictionary[self.unknown_word_symbol])\n","\n","    # 入力された単語を one-hot ベクトルにする\n","    def to_one_hot(self, word):\n","        index = self.get_word_id(word)\n","        data = np.zeros(self.vocabulary_size)\n","        data[index] = 1\n","        return data\n","\n","    def seq_to_matrix(self, seq):\n","        print(seq)\n","        data = np.array([self.to_one_hot(word) for word in seq]) # (data_n, vocabulary_size)\n","        return data.transpose() # (vocabulary_size, data_n)\n","\n","class Language:\n","    \"\"\"\n","    input layer: self.vocabulary_size\n","    hidden layer: rnn_size = 30\n","    output layer: self.vocabulary_size\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.corpus = Corpus()\n","        self.dictionary = self.corpus.dictionary\n","        self.vocabulary_size = len(self.dictionary) # 単語数\n","        self.input_layer_size = self.vocabulary_size # 入力層の数\n","        self.hidden_layer_size = 30 # 隠れ層の RNN ユニットの数\n","        self.output_layer_size = self.vocabulary_size # 出力層の数\n","        self.batch_size = 128 # バッチサイズ\n","        self.chunk_size = 5 # 展開するシーケンスの数。c_0, c_1, ..., c_(chunk_size - 1) を入力し、c_(chunk_size) 番目の単語の確率が出力される。\n","        self.learning_rate = 0.005 # 学習率\n","        self.epochs = 1000 # 学習するエポック数\n","        self.forget_bias = 1.0 # LSTM における忘却ゲートのバイアス\n","        self.model_filename = \"./data_for_predict/predict_model.ckpt\"\n","        self.unknown_word_symbol = self.corpus.unknown_word_symbol\n","\n","    def inference(self, input_data, initial_state):\n","        \"\"\"\n","        :param input_data: (batch_size, chunk_size, vocabulary_size) 次元のテンソル\n","        :param initial_state: (batch_size, hidden_layer_size) 次元の行列\n","        :return:\n","        \"\"\"\n","        # 重みとバイアスの初期化\n","        hidden_w = tf.Variable(tf.truncated_normal([self.input_layer_size, self.hidden_layer_size], stddev=0.01))\n","        hidden_b = tf.Variable(tf.ones([self.hidden_layer_size]))\n","        output_w = tf.Variable(tf.truncated_normal([self.hidden_layer_size, self.output_layer_size], stddev=0.01))\n","        output_b = tf.Variable(tf.ones([self.output_layer_size]))\n","\n","        # BasicLSTMCell, BasicRNNCell は (batch_size, hidden_layer_size) が chunk_size 数ぶんつながったリストを入力とする。\n","        # 現時点での入力データは (batch_size, chunk_size, input_layer_size) という３次元のテンソルなので\n","        # tf.transpose や tf.reshape などを駆使してテンソルのサイズを調整する。\n","\n","        input_data = tf.transpose(input_data, [1, 0, 2]) # 転置。(chunk_size, batch_size, vocabulary_size)\n","        input_data = tf.reshape(input_data, [-1, self.input_layer_size]) # 変形。(chunk_size * batch_size, input_layer_size)\n","        input_data = tf.matmul(input_data, hidden_w) + hidden_b # 重みWとバイアスBを適用。 (chunk_size, batch_size, hidden_layer_size)\n","        input_data = tf.split(input_data, self.chunk_size, 0) # リストに分割。chunk_size * (batch_size, hidden_layer_size)\n","\n","        # RNN のセルを定義する。RNN Cell の他に LSTM のセルや GRU のセルなどが利用できる。\n","        cell = tf.nn.rnn_cell.BasicRNNCell(self.hidden_layer_size)\n","        outputs, states = tf.nn.static_rnn(cell, input_data, initial_state=initial_state)\n","        \n","        # 最後に隠れ層から出力層につながる重みとバイアスを処理する\n","        # 最終的に softmax 関数で処理し、確率として解釈される。\n","        # softmax 関数はこの関数の外で定義する。\n","        output = tf.matmul(outputs[-1], output_w) + output_b\n","\n","        return output\n","\n","    def loss(self, logits, labels):\n","        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n","\n","        return cost\n","\n","    def training(self, cost):\n","        # 今回は最適化手法として Adam を選択する。\n","        # ここの AdamOptimizer の部分を変えることで、Adagrad, Adadelta などの他の最適化手法を選択することができる\n","        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(cost)\n","\n","        return optimizer\n","\n","    def train(self):\n","        # 変数などの用意\n","        input_data = tf.placeholder(\"float\", [None, self.chunk_size, self.input_layer_size])\n","        actual_labels = tf.placeholder(\"float\", [None, self.output_layer_size])\n","        initial_state = tf.placeholder(\"float\", [None, self.hidden_layer_size])\n","\n","        prediction = self.inference(input_data, initial_state)\n","        cost = self.loss(prediction, actual_labels)\n","        optimizer = self.training(cost)\n","        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(actual_labels, 1))\n","        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","        # TensorBoard で可視化するため、クロスエントロピーをサマリーに追加\n","        tf.summary.scalar(\"Cross entropy: \", cost)\n","        summary = tf.summary.merge_all()\n","\n","        # 訓練・テストデータの用意\n","        # corpus = Corpus()\n","        trX, trY, teX, teY = self.corpus.prepare_data()\n","        training_num = trX.shape[0]\n","\n","        # ログを保存するためのディレクトリ\n","        timestamp = time.time()\n","        dirname = datetime.datetime.fromtimestamp(timestamp).strftime(\"%Y%m%d%H%M%S\")\n","\n","        # ここから実際に学習を走らせる\n","        with tf.Session() as sess:\n","            sess.run(tf.global_variables_initializer())\n","            summary_writer = tf.summary.FileWriter(\"./log/\" + dirname, sess.graph)\n","\n","            # エポックを回す\n","            for epoch in range(self.epochs):\n","                step = 0\n","                epoch_loss = 0\n","                epoch_acc = 0\n","\n","                # 訓練データをバッチサイズごとに分けて学習させる (= optimizer を走らせる)\n","                # エポックごとの損失関数の合計値や（訓練データに対する）精度も計算しておく\n","                while (step + 1) * self.batch_size < training_num:\n","                    start_idx = step * self.batch_size\n","                    end_idx = (step + 1) * self.batch_size\n","\n","                    batch_xs = trX[start_idx:end_idx, :, :]\n","                    batch_ys = trY[start_idx:end_idx, :]\n","\n","                    _, c, a = sess.run([optimizer, cost, accuracy],\n","                                       feed_dict={input_data: batch_xs,\n","                                                  actual_labels: batch_ys,\n","                                                  initial_state: np.zeros([self.batch_size, self.hidden_layer_size])\n","                                                  }\n","                                       )\n","                    epoch_loss += c\n","                    epoch_acc += a\n","                    step += 1\n","\n","                # コンソールに損失関数の値や精度を出力しておく\n","                print(\"Epoch\", epoch, \"completed ouf of\", self.epochs, \"-- loss:\", epoch_loss, \" -- accuracy:\",\n","                      epoch_acc / step)\n","\n","                # Epochが終わるごとにTensorBoard用に値を保存\n","                summary_str = sess.run(summary, feed_dict={input_data: trX,\n","                                                           actual_labels: trY,\n","                                                           initial_state: np.zeros(\n","                                                               [trX.shape[0],\n","                                                                self.hidden_layer_size]\n","                                                           )\n","                                                           }\n","                                       )\n","                summary_writer.add_summary(summary_str, epoch)\n","                summary_writer.flush()\n","\n","            # 学習したモデルも保存しておく\n","            saver = tf.train.Saver()\n","            saver.save(sess, self.model_filename)\n","\n","            # 最後にテストデータでの精度を計算して表示する\n","            a = sess.run(accuracy, feed_dict={input_data: teX, actual_labels: teY,\n","                                              initial_state: np.zeros([teX.shape[0], self.hidden_layer_size])})\n","            print(\"Accuracy on test:\", a)\n","\n","\n","    def predict(self, seq):\n","        \"\"\"\n","        文章を入力したときに次に来る単語を予測する\n","        :param seq: 予測したい単語の直前の文字列。chunk_size 以上の単語数が必要。\n","        :return:\n","        \"\"\"\n","\n","        # 最初に復元したい変数をすべて定義してしまいます\n","        tf.reset_default_graph()\n","        input_data = tf.placeholder(\"float\", [None, self.chunk_size, self.input_layer_size])\n","        initial_state = tf.placeholder(\"float\", [None, self.hidden_layer_size])\n","        prediction = tf.nn.softmax(self.inference(input_data, initial_state))\n","        predicted_labels = tf.argmax(prediction, 1)\n","\n","        # 入力データの作成\n","        # seq を one-hot 表現に変換する。\n","        words = [word for word in seq.split() if not word.startswith(\"-\")]\n","        x = np.zeros([1, self.chunk_size, self.input_layer_size])\n","        for i in range(self.chunk_size):\n","            word = seq[len(words) - self.chunk_size + i]\n","            index = self.dictionary.get(word, self.dictionary[self.unknown_word_symbol])\n","            x[0][i][index] = 1\n","        feed_dict = {\n","            input_data: x, # (1, chunk_size, vocabulary_size)\n","            initial_state: np.zeros([1, self.hidden_layer_size])\n","        }\n","\n","        # tf.Session()を用意\n","        with tf.Session() as sess:\n","            # 保存したモデルをロードする。ロード前にすべての変数を用意しておく必要がある。\n","            saver = tf.train.Saver()\n","            saver.restore(sess, self.model_filename)\n","\n","            # ロードしたモデルを使って予測結果を計算\n","            u, v = sess.run([prediction, predicted_labels], feed_dict=feed_dict)\n","\n","            keys = list(self.dictionary.keys())\n","\n","\n","            # コンソールに文字ごとの確率を表示\n","            for i in range(self.vocabulary_size):\n","                c = self.unknown_word_symbol if i == (self.vocabulary_size - 1) else keys[i]\n","                print(c, \":\", u[0][i])\n","\n","            print(\"Prediction:\", seq + \" \" + (\"<???>\" if v[0] == (self.vocabulary_size - 1) else keys[v[0]]))\n","\n","        return u[0]\n","\n","\n","def build_dict():\n","    cp = Corpus()\n","    cp.build_dict()\n","\n","if __name__ == \"__main__\":\n","    #build_dict()\n","\n","    ln = Language()\n","\n","    # 学習するときに呼び出す\n","    #ln.train()\n","\n","    # 保存したモデルを使って単語の予測をする\n","    ln.predict(\"some of them looks like\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ScvHDB2C2SkE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}