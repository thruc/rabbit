
import sys, os
sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定
import numpy as np
from data.mnist import load_mnist
from PIL import Image
import pickle
from common import functions
import matplotlib.pyplot as plt

# mnistをロード
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

# 初期設定
def init_network():
    network = {}
    wieght_init = 1
    network['W1'] = wieght_init * np.random.rand(784, 50)
    network['W2'] = wieght_init * np.random.rand(50, 10)
    network['b1'] = np.zeros(50)
    network['b2'] = np.zeros(10)

    return network

# 順伝播
def forward(network, x):
    W1, W2 = network['W1'], network['W2']
    b1, b2  = network['b1'], network['b2']
        
    u1 =  np.dot(x, W1) + b1
    z1 = functions.sigmoid(u1)
    u2 =  np.dot(z1, W2) + b2
    y = functions.softmax(u2)
 
    return z1, y

# 誤差逆伝播
def backward(x, t, z1, y):
    grad = {}
    
    W1, W2 = network['W1'], network['W2']
    b1, b2 = network['b1'], network['b2']    
    # 出力層でのデルタ
    delta2 = functions.d_softmax_with_loss(t, y)
    # b2の勾配
    grad['b2'] = np.sum(delta2, axis=0)
    # W2の勾配
    grad['W2'] = np.dot(z1.T, delta2)
    # 1層でのデルタ
    delta1 = np.dot(delta2, W2.T) * functions.d_sigmoid(z1)
    # b1の勾配
    grad['b1'] = np.sum(delta1, axis=0)
    # W1の勾配
    grad['W1'] = np.dot(x.T, delta1)

    return grad


network = init_network()
# iters_num = 1
iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
# 学習率
learning_rate = 0.1
accuracy_cnt = 0
accuracies = []
# for i in range(len(x_train)):
#     y, z1, z2 = forward(network, x_train[i])
#     p = np.argmax(y) # 最も確率の高い要素のインデックスを取得
#     if p == t_train[i]:
#         accuracy_cnt += 1

# print("Accuracy:" + str(float(accuracy_cnt) / len(x_train)))

def accuracy(x, t):
    z1, y = forward(network, x)

    y = np.argmax(y, axis=1)
    if t.ndim != 1 : t = np.argmax(t, axis=1)

    accuracy = np.sum(y == t) / float(x.shape[0])
    return accuracy
 
for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 出力
    z1, y = forward(network, x_batch)
    
    # 誤差
    grad = backward(x_batch, t_batch, z1, y)
    #print(grad)
    # print(t_batch.shape[0])
    
    p = np.argmax(y) # 最も確率の高い要素のインデックスを取得
    
    accuracies.append(accuracy(x_batch, t_batch))
#     print(accuracy(x_batch, t_batch))
#     if i % batch_size == 0:
#         loss = functions.cross_entropy_error(t_batch, y)
#         print(loss)
        
#         print(t_batch[batch_size - 1].shape)
#         print(y[batch_size - 1].shape)
#         print(t_batch[batch_size - 1])
#         print(y[batch_size - 1])

#     print(grad)

    for key in ('W1', 'W2', 'b1', 'b2'):
        network[key]  -= learning_rate * grad[key]
#         print(network[key])


lists = range(iters_num)

print(len(accuracies))
plt.plot(lists, accuracies)
plt.show()


